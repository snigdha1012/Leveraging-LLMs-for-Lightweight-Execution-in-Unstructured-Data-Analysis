{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T20:23:13.675655Z",
     "start_time": "2025-11-20T20:19:36.356256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Try importing Ollama SDK\n",
    "try:\n",
    "    from ollama import chat\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"‚ö† Ollama SDK not installed. Install via: pip install ollama\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset (100 samples)\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"imdb\", split=\"test\").select(range(100))\n",
    "queries = dataset['text']\n",
    "\n",
    "# -----------------------------\n",
    "# Measure ONLY Ollama runtime\n",
    "# -----------------------------\n",
    "def measure_ollama_runtime(queries):\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        print(\"‚ö† Ollama not available, skipping timing.\")\n",
    "        return\n",
    "\n",
    "    print(\"‚è≥ Measuring Ollama LLM runtime...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        prompt = f\"Say 'OK' for this input:\\n\\\"{q}\\\"\"\n",
    "        try:\n",
    "            chat(model=\"qwen2.5:7b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Ollama error on query {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    total_time = round(time.time() - start, 2)\n",
    "    print(f\"\\n‚è±Ô∏è Total Ollama Runtime for {len(queries)} queries: {total_time} seconds\")\n",
    "    print(f\"‚úÖ Average per query: {round(total_time/len(queries), 2)} seconds/query\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    measure_ollama_runtime(queries)\n"
   ],
   "id": "c30106ec44308bc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring Ollama LLM runtime...\n",
      "\n",
      " Total Ollama Runtime for 100 queries: 215.71 seconds\n",
      "Average per query: 2.16 seconds/query\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T18:32:43.531810Z",
     "start_time": "2025-11-21T18:32:09.918460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Suppress warnings/logs\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Sentiment models\n",
    "# -----------------------------\n",
    "SENTIMENT_MODELS = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    \"textattack/bert-base-uncased-SST-2\",\n",
    "    \"roberta-large-mnli\",\n",
    "    \"siebert/sentiment-roberta-large-english\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Detect device\n",
    "# -----------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available ‚Äî using CPU\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset (100 samples)\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"imdb\", split=\"test\").select(range(100))\n",
    "queries = dataset['text']\n",
    "true_labels = dataset['label']\n",
    "\n",
    "# -----------------------------\n",
    "# Run model on MPS/CPU\n",
    "# -----------------------------\n",
    "def run_model_mps(model_name, queries):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for q in queries:\n",
    "        encoded = tokenizer(q, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "            pred_label = output.logits.argmax(dim=1).item()\n",
    "        predictions.append(pred_label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Run all models and compute accuracy + latency\n",
    "# -----------------------------\n",
    "def run_all_models(queries, true_labels):\n",
    "    results = []\n",
    "\n",
    "    for model_name in SENTIMENT_MODELS:\n",
    "        #print(f\"\\n‚è≥ Running {model_name} ...\")\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            preds = run_model_mps(model_name, queries)\n",
    "            acc = round(accuracy_score(true_labels, preds) * 100, 2)\n",
    "            latency = round(time.time() - start, 2)\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"latency\": latency\n",
    "            })\n",
    "\n",
    "            #print(f\"‚úî {model_name} -> Accuracy: {acc}%, Latency: {latency}s\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model failed: {model_name}, Error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_all_models(queries, true_labels)\n",
    "\n",
    "    print(\"\\nüìä Final Model Results:\")\n",
    "    for r in results:\n",
    "        print(r)\n"
   ],
   "id": "6234564012e32e1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "\n",
      "üìä Final Model Results:\n",
      "{'model': 'distilbert-base-uncased-finetuned-sst-2-english', 'accuracy': 89.0, 'latency': 2.33}\n",
      "{'model': 'finiteautomata/bertweet-base-sentiment-analysis', 'accuracy': 59.0, 'latency': 2.64}\n",
      "{'model': 'textattack/bert-base-uncased-SST-2', 'accuracy': 84.0, 'latency': 4.06}\n",
      "{'model': 'roberta-large-mnli', 'accuracy': 10.0, 'latency': 12.02}\n",
      "{'model': 'siebert/sentiment-roberta-large-english', 'accuracy': 95.0, 'latency': 11.85}\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T18:37:01.191890Z",
     "start_time": "2025-11-21T18:36:16.841264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Try loading Ollama SDK\n",
    "try:\n",
    "    from ollama import chat\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama SDK not installed. Ollama recommendations will be skipped.\")\n",
    "\n",
    "# Suppress warnings/logs\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Sentiment models\n",
    "# -----------------------------\n",
    "SENTIMENT_MODELS = [\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "    \"textattack/bert-base-uncased-SST-2\",\n",
    "    \"roberta-large-mnli\",\n",
    "    \"siebert/sentiment-roberta-large-english\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Label map\n",
    "# -----------------------------\n",
    "LABEL_MAP = {\n",
    "    \"LABEL_0\": 0, \"LABEL_1\": 1,\n",
    "    \"1 star\": 0, \"2 stars\": 0, \"3 stars\": 0,\n",
    "    \"4 stars\": 1, \"5 stars\": 1\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Detect device\n",
    "# -----------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available ‚Äî using CPU\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"imdb\", split=\"test\").select(range(100))  # smaller sample\n",
    "queries = dataset['text']\n",
    "true_labels = dataset['label']\n",
    "\n",
    "# -----------------------------\n",
    "# MPS-safe model inference\n",
    "# -----------------------------\n",
    "def run_model_mps(model_name, queries):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    for q in queries:\n",
    "        encoded = tokenizer(q, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "            pred_label = output.logits.argmax(dim=1).item()\n",
    "        predictions.append(pred_label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# -----------------------------\n",
    "# Run all models and measure accuracy/latency with early stopping\n",
    "# -----------------------------\n",
    "def run_batch_sentiment_analysis(queries, true_labels, query_type=\"accuracy\", target_acc=0):\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_latency = float('inf')\n",
    "\n",
    "    for model_name in SENTIMENT_MODELS:\n",
    "        #print(f\"\\n‚è≥ Running {model_name} ...\")\n",
    "        try:\n",
    "            start = time.time()\n",
    "            preds = run_model_mps(model_name, queries)\n",
    "            acc = round(accuracy_score(true_labels, preds) * 100, 2)\n",
    "            latency = round(time.time() - start, 2)\n",
    "\n",
    "            # -----------------------------\n",
    "            # Early stopping logic\n",
    "            # -----------------------------\n",
    "            skip_model = False\n",
    "\n",
    "            if query_type == \"accuracy\":\n",
    "                # Skip if accuracy worse than current best or same accuracy but slower\n",
    "                if acc < best_acc or (acc == best_acc and latency > best_latency):\n",
    "                    skip_model = True\n",
    "            elif query_type == \"latency\":\n",
    "                # Skip if below target accuracy\n",
    "                if acc < target_acc:\n",
    "                    skip_model = True\n",
    "                # Also skip if same accuracy but slower than previous best\n",
    "                elif acc == best_acc and latency > best_latency:\n",
    "                    skip_model = True\n",
    "\n",
    "            if skip_model:\n",
    "                #print(f\"Skipping {model_name} ‚Äî worse than current best\")\n",
    "                continue\n",
    "\n",
    "            # Keep this model\n",
    "            results.append({\"model\": model_name, \"accuracy\": acc, \"latency\": latency})\n",
    "            #print(f\"{model_name} -> Accuracy: {acc}%, Latency: {latency}s\")\n",
    "\n",
    "            # Update best metrics\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_latency = latency\n",
    "            elif acc == best_acc:\n",
    "                best_latency = min(best_latency, latency)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model failed: {model_name}, Error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Ollama selection\n",
    "# -----------------------------\n",
    "def ollama_select_model(results, query_type=\"accuracy\", target_acc=0):\n",
    "    if not OLLAMA_AVAILABLE or not results:\n",
    "        return None\n",
    "\n",
    "    prompt = \"You are a model selection assistant.\\n\"\n",
    "    prompt += \"Here are model results:\\n\"\n",
    "    for r in results:\n",
    "        prompt += f\"{r['model']}: accuracy={r['accuracy']} latency={r['latency']}\\n\"\n",
    "\n",
    "    if query_type == \"accuracy\":\n",
    "        prompt += \"\\nSelect the best model based on highest accuracy. Break ties using lowest latency. Reply with the model name and reason for selecting the model.\"\n",
    "    elif query_type == \"latency\":\n",
    "        prompt += f\"\\nSelect the fastest model that has accuracy >= {target_acc}%. If multiple, pick the one with the lowest latency. Reply with the model name and reason for selecting the model.\"\n",
    "\n",
    "    try:\n",
    "        response = chat(model=\"qwen2.5:7b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response.message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Ollama error:\", e)\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # User query\n",
    "    print(\"\\nQuery options:\")\n",
    "    print(\"1 - Most accurate model\")\n",
    "    print(\"2 - Fastest model at target accuracy\")\n",
    "    query_input = input(\"Enter your query (1 or 2): \").strip()\n",
    "\n",
    "    if query_input == \"1\":\n",
    "        query_type = \"accuracy\"\n",
    "        target_acc = 0\n",
    "    elif query_input == \"2\":\n",
    "        query_type = \"latency\"\n",
    "        try:\n",
    "            target_acc = float(input(\"Enter target accuracy (e.g., 70 for 70%): \"))\n",
    "        except:\n",
    "            target_acc = 0\n",
    "    else:\n",
    "        query_type = \"accuracy\"\n",
    "        target_acc = 0\n",
    "\n",
    "    # Run models\n",
    "    results = run_batch_sentiment_analysis(queries, true_labels, query_type, target_acc)\n",
    "\n",
    "    # Ollama selection (use filtered results)\n",
    "    selected_model = ollama_select_model(results, query_type, target_acc)\n",
    "    if selected_model:\n",
    "        print(\"\\nOllama Selected Model:\", selected_model)\n",
    "    else:\n",
    "        print(\"\\nOllama selection failed. You may fallback to Python selection.\")\n"
   ],
   "id": "341cc079a5a13b91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "\n",
      "Query options:\n",
      "1 - Most accurate model\n",
      "2 - Fastest model at target accuracy\n",
      "\n",
      "Ollama Selected Model: Based on the provided model results, the sentiment analysis model \"siebert/sentiment-roberta-large-english\" is selected as the best model.\n",
      "\n",
      "**Reason:**\n",
      "The model \"siebert/sentiment-roberta-large-english\" has a higher accuracy of 95.0 compared to \"distilbert-base-uncased-finetuned-sst-2-english,\" which has an accuracy of 89.0. Since there is a clear difference in accuracy, we do not need to consider latency for this selection.\n",
      "\n",
      "Therefore, the model \"siebert/sentiment-roberta-large-english\" is chosen due to its superior accuracy.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
