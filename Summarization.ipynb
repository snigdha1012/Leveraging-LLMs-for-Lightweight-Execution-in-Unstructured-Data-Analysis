{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T02:53:03.775586Z",
     "start_time": "2025-11-29T02:50:49.909651Z"
    }
   },
   "source": [
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "try:\n",
    "    from ollama import chat\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama SDK missing: pip install ollama\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# HuggingFace Text Generation Models\n",
    "# ------------------------------------------------\n",
    "GENERATION_MODELS = [\n",
    "    \"t5-small\",\n",
    "    \"google/mt5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Detect device\n",
    "# ------------------------------------------------\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Load dataset (example: english summarization)\n",
    "# ------------------------------------------------\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:25]\")\n",
    "\n",
    "inputs = dataset[\"article\"]\n",
    "references = dataset[\"highlights\"]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Generation function\n",
    "# ------------------------------------------------\n",
    "def run_model_generate(model_name, inputs, max_new_tokens=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for text in inputs:\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        outputs.append(generated)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Evaluation: BLEU + ROUGE\n",
    "# ------------------------------------------------\n",
    "def evaluate_generation(preds, refs):\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    for p, r in zip(preds, refs):\n",
    "        bleu_scores.append(sentence_bleu([r.split()], p.split()))\n",
    "        rouge = scorer.score(r, p)\n",
    "        rouge_scores.append(rouge[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": round(sum(bleu_scores) / len(bleu_scores) * 100, 2),\n",
    "        \"rougeL\": round(sum(rouge_scores) / len(rouge_scores) * 100, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Unified model runner\n",
    "# ------------------------------------------------\n",
    "def run_any_generation(model_name, inputs):\n",
    "    return run_model_generate(model_name, inputs)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Batch evaluation\n",
    "# ------------------------------------------------\n",
    "def run_batch_generation(inputs, references, query_type=\"quality\", target_bleu=0):\n",
    "    results = []\n",
    "    best_bleu = 0\n",
    "    best_latency = float('inf')\n",
    "\n",
    "    for model_name in GENERATION_MODELS:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            preds = run_any_generation(model_name, inputs)\n",
    "            metrics = evaluate_generation(preds, references)\n",
    "            latency = round(time.time() - start, 2)\n",
    "\n",
    "            bleu = metrics[\"bleu\"]\n",
    "            rouge = metrics[\"rougeL\"]\n",
    "\n",
    "            skip = False\n",
    "            if query_type == \"quality\":\n",
    "                if bleu < best_bleu or (bleu == best_bleu and latency > best_latency):\n",
    "                    skip = True\n",
    "            else:\n",
    "                # latency mode\n",
    "                if bleu < target_bleu:\n",
    "                    skip = True\n",
    "\n",
    "            if skip:\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"bleu\": bleu,\n",
    "                \"rougeL\": rouge,\n",
    "                \"latency\": latency\n",
    "            })\n",
    "\n",
    "            if bleu > best_bleu or (bleu == best_bleu and latency < best_latency):\n",
    "                best_bleu = bleu\n",
    "                best_latency = latency\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model failed: {model_name} | Error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Ollama: Select best model\n",
    "# ------------------------------------------------\n",
    "def ollama_select_model(results, query_type=\"quality\", target_bleu=0):\n",
    "    if not OLLAMA_AVAILABLE or not results:\n",
    "        return None\n",
    "\n",
    "    prompt = \"You are a model selection assistant.\\nHere are text generation model stats:\\n\"\n",
    "    for r in results:\n",
    "        prompt += (\n",
    "            f\"{r['model']}: BLEU={r['bleu']} ROUGE-L={r['rougeL']} \"\n",
    "            f\"latency={r['latency']} sec\\n\"\n",
    "        )\n",
    "\n",
    "    if query_type == \"quality\":\n",
    "        prompt += \"\\nSelect the highest BLEU model (tie → lowest latency). Return ONLY the model name.\"\n",
    "    else:\n",
    "        prompt += f\"\\nSelect fastest model with BLEU >= {target_bleu}. Return ONLY the model name.\"\n",
    "\n",
    "    try:\n",
    "        response = chat(model=\"qwen2.5:7b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response.message.content.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Main entry\n",
    "# ------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nQuery options:\")\n",
    "    print(\"1 - Best quality model (BLEU)\")\n",
    "    print(\"2 - Fastest model with min BLEU target\")\n",
    "\n",
    "    choice = input(\"Enter (1 or 2): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query_type = \"quality\"\n",
    "        target_bleu = 0\n",
    "    else:\n",
    "        query_type = \"latency\"\n",
    "        target_bleu = float(input(\"Enter target BLEU score (0–100): \"))\n",
    "\n",
    "    results = run_batch_generation(inputs, references, query_type, target_bleu)\n",
    "    selected_model = ollama_select_model(results, query_type, target_bleu)\n",
    "\n",
    "    print(\"\\nOllama Selected Model:\", selected_model)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Query options:\n",
      "1 - Best quality model (BLEU)\n",
      "2 - Fastest model with min BLEU target\n",
      "\n",
      "Ollama Selected Model: None\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
