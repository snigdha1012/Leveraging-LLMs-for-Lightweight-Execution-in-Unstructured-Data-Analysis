{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:02:51.767137Z",
     "start_time": "2025-12-06T03:00:36.152699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    from ollama import chat\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama SDK missing: pip install ollama\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Model list\n",
    "GENERATION_MODELS = [\n",
    "    \"t5-small\",\n",
    "    \"google/mt5-small\",\n",
    "    \"facebook/bart-base\"\n",
    "]\n",
    "\n",
    "\n",
    "# Detect device\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:25]\")\n",
    "\n",
    "inputs = dataset[\"article\"]\n",
    "references = dataset[\"highlights\"]\n",
    "\n",
    "# Text generation\n",
    "def run_model_generate(model_name, inputs, max_new_tokens=128):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    for text in inputs:\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(**enc, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        outputs.append(generated)\n",
    "    return outputs\n",
    "\n",
    "# Evaluation (BLEU + ROUGE)\n",
    "def evaluate_generation(preds, refs):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    bleu_scores, rouge_scores = [], []\n",
    "\n",
    "    for p, r in zip(preds, refs):\n",
    "        bleu_scores.append(sentence_bleu([r.split()], p.split()))\n",
    "        rouge_scores.append(scorer.score(r, p)[\"rougeL\"].fmeasure)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": round(sum(bleu_scores) / len(bleu_scores) * 100, 2),\n",
    "        \"rougeL\": round(sum(rouge_scores) / len(rouge_scores) * 100, 2)\n",
    "    }\n",
    "\n",
    "# Run all models\n",
    "def run_batch_generation(inputs, references, query_type=\"quality\", target_bleu=0):\n",
    "    results = []\n",
    "    best_bleu = 0\n",
    "    best_latency = float('inf')\n",
    "\n",
    "    for model_name in GENERATION_MODELS:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            preds = run_model_generate(model_name, inputs)\n",
    "            metrics = evaluate_generation(preds, references)\n",
    "            latency = round(time.time() - start, 2)\n",
    "\n",
    "            bleu = metrics[\"bleu\"]\n",
    "            rouge = metrics[\"rougeL\"]\n",
    "\n",
    "            skip = False\n",
    "            if query_type == \"quality\":\n",
    "                if bleu < best_bleu or (bleu == best_bleu and latency > best_latency):\n",
    "                    skip = True\n",
    "            else:  # latency mode\n",
    "                if bleu < target_bleu:\n",
    "                    skip = True\n",
    "\n",
    "            if skip:\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"bleu\": bleu,\n",
    "                \"rougeL\": rouge,\n",
    "                \"latency\": latency\n",
    "            })\n",
    "\n",
    "            if bleu > best_bleu or (bleu == best_bleu and latency < best_latency):\n",
    "                best_bleu = bleu\n",
    "                best_latency = latency\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model failed: {model_name} | Error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Build FAISS index from benchmarks_textgen.csv\n",
    "def build_vector_db(csv_path=\"benchmarks_s.csv\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    docs = [\n",
    "        f\"Model: {r['model']}. Dataset: {r['dataset']}. BLEU: {r['bleu']}. ROUGE: {r['rouge']}. Latency: {r['latency']}.\"\n",
    "        for _, r in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    embeddings = embedder.encode(docs, show_progress_bar=False)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings, dtype=\"float32\"))\n",
    "\n",
    "    return df, index, embedder, docs\n",
    "\n",
    "\n",
    "# RAG retrieval\n",
    "def query_benchmark_rag(task_query, df, index, embedder, docs, k=5):\n",
    "    q_emb = embedder.encode([task_query]).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return \"\\n\".join([docs[i] for i in I[0]])\n",
    "\n",
    "# Ollama LLM model selection using RAG\n",
    "def ollama_select_model(results, rag_context, query_type=\"quality\", target_bleu=0):\n",
    "    if not OLLAMA_AVAILABLE:\n",
    "        return None\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        You are a strict model selector.\n",
    "\n",
    "        Retrieved benchmark evidence:\n",
    "        {rag_context}\n",
    "\n",
    "        Evaluation results:\n",
    "        {json.dumps(results, indent=2)}\n",
    "\n",
    "        Rules:\n",
    "        - If goal = quality → pick highest BLEU (tie → lowest latency)\n",
    "        - If goal = latency → pick lowest latency among models with BLEU ≥ target\n",
    "        - Output MUST be only the model name.\n",
    "\n",
    "        Goal: {query_type}\n",
    "        Target BLEU: {target_bleu}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = chat(model=\"qwen2.5:7b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response.message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Ollama error:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nQuery options:\")\n",
    "    print(\"1 - Highest quality model (BLEU)\")\n",
    "    print(\"2 - Fastest model with minimum BLEU\")\n",
    "\n",
    "    choice = input(\"Enter (1 or 2): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query_type = \"quality\"\n",
    "        target_bleu = 0\n",
    "    else:\n",
    "        query_type = \"latency\"\n",
    "        target_bleu = float(input(\"Enter minimum BLEU target: \"))\n",
    "\n",
    "    # Run evaluations\n",
    "    results = run_batch_generation(inputs, references, query_type, target_bleu)\n",
    "\n",
    "    # Build RAG DB\n",
    "    df, index, embedder, docs = build_vector_db()\n",
    "\n",
    "    rag_context = query_benchmark_rag(\n",
    "        \"best summarization model for cnn dailymail\",\n",
    "        df, index, embedder, docs\n",
    "    )\n",
    "\n",
    "    # LLM selector\n",
    "    selected_model = ollama_select_model(results, rag_context, query_type, target_bleu)\n",
    "\n",
    "    print(\"\\n=== OLLAMA SELECTED MODEL ===\")\n",
    "    print(selected_model)\n"
   ],
   "id": "db3e442e7be16944",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Query options:\n",
      "1 - Highest quality model (BLEU)\n",
      "2 - Fastest model with minimum BLEU\n",
      "\n",
      "=== OLLAMA SELECTED MODEL ===\n",
      "facebook/bart-base\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
