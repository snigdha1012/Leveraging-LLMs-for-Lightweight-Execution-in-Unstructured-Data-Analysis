{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T02:18:05.067627Z",
     "start_time": "2025-11-29T02:17:39.600084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "try:\n",
    "    import langid\n",
    "    LANGID_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LANGID_AVAILABLE = False\n",
    "    print(\"langid.py not installed. Install using: pip install langid\")\n",
    "\n",
    "try:\n",
    "    from ollama import chat\n",
    "    OLLAMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OLLAMA_AVAILABLE = False\n",
    "    print(\"Ollama SDK not installed. Ollama recommendations will be skipped.\")\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# HuggingFace language models\n",
    "# -----------------------------\n",
    "LANGUAGE_MODELS = [\n",
    "    \"papluca/xlm-roberta-base-language-detection\",\n",
    "    \"Joshi-Aryan/Fine_Tuned_HF_Language_Identification_Model\",\n",
    "    \"langid.py\",\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Detect device\n",
    "# -----------------------------\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"mps\")\n",
    "    if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load multi-language dataset\n",
    "# -----------------------------\n",
    "languages = [\"en\", \"fr\", \"es\", \"de\"]\n",
    "texts = []\n",
    "true_labels = []\n",
    "\n",
    "for lang in languages:\n",
    "    dataset = load_dataset(\"wiki40b\", lang, split=\"train[:25]\")\n",
    "    texts.extend(dataset['text'])\n",
    "    true_labels.extend([lang] * len(dataset))\n",
    "\n",
    "# -----------------------------\n",
    "# HF model inference\n",
    "# -----------------------------\n",
    "def run_model_hf(model_name, texts):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "    for t in texts:\n",
    "        encoded = tokenizer(t, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "            pred_id = output.logits.argmax(dim=1).item()\n",
    "            pred_label = model.config.id2label[pred_id]\n",
    "        preds.append(pred_label.lower()[:2])\n",
    "    return preds\n",
    "\n",
    "# -----------------------------\n",
    "# langid.py inference\n",
    "# -----------------------------\n",
    "def run_langid(texts):\n",
    "    if not LANGID_AVAILABLE:\n",
    "        raise RuntimeError(\"langid.py not installed. Install using: pip install langid\")\n",
    "    return [langid.classify(t)[0] for t in texts]\n",
    "\n",
    "# -----------------------------\n",
    "# Unified model runner\n",
    "# -----------------------------\n",
    "def run_any_model(model_name, texts):\n",
    "    if model_name == \"langid.py\":\n",
    "        return run_langid(texts)\n",
    "    else:\n",
    "        return run_model_hf(model_name, texts)\n",
    "\n",
    "# -----------------------------\n",
    "# Run all models with accuracy/latency\n",
    "# -----------------------------\n",
    "def run_batch_language_detection(texts, true_labels, query_type=\"accuracy\", target_acc=0):\n",
    "    results = []\n",
    "    best_acc = 0\n",
    "    best_latency = float('inf')\n",
    "\n",
    "    for model_name in LANGUAGE_MODELS:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            preds = run_any_model(model_name, texts)\n",
    "            acc = round(accuracy_score(true_labels, preds) * 100, 2)\n",
    "            latency = round(time.time() - start, 2)\n",
    "\n",
    "            skip = False\n",
    "            if query_type == \"accuracy\":\n",
    "                if acc < best_acc or (acc == best_acc and latency > best_latency):\n",
    "                    skip = True\n",
    "            else:  # latency mode\n",
    "                if acc < target_acc:\n",
    "                    skip = True\n",
    "\n",
    "            if skip:\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"accuracy\": acc,\n",
    "                \"latency\": latency\n",
    "            })\n",
    "\n",
    "            if acc > best_acc or (acc == best_acc and latency < best_latency):\n",
    "                best_acc = acc\n",
    "                best_latency = latency\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Model failed: {model_name}, Error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# -----------------------------\n",
    "# Ollama selection\n",
    "# -----------------------------\n",
    "def ollama_select_model(results, query_type=\"accuracy\", target_acc=0):\n",
    "    if not OLLAMA_AVAILABLE or not results:\n",
    "        return None\n",
    "\n",
    "    prompt = \"You are a model selection assistant.\\nHere are model results:\\n\"\n",
    "    for r in results:\n",
    "        prompt += f\"{r['model']}: accuracy={r['accuracy']} latency={r['latency']}\\n\"\n",
    "\n",
    "    if query_type == \"accuracy\":\n",
    "        prompt += \"\\nSelect the best model by accuracy (tie-break: latency). Return ONLY the model name.\"\n",
    "    else:\n",
    "        prompt += f\"\\nSelect fastest model with accuracy >= {target_acc}%. Return ONLY the model name.\"\n",
    "\n",
    "    try:\n",
    "        response = chat(model=\"qwen2.5:7b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return response.message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Ollama error:\", e)\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nQuery options:\")\n",
    "    print(\"1 - Most accurate model\")\n",
    "    print(\"2 - Fastest model at target accuracy\")\n",
    "\n",
    "    choice = input(\"Enter (1 or 2): \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        query_type = \"accuracy\"\n",
    "        target_acc = 0\n",
    "    else:\n",
    "        query_type = \"latency\"\n",
    "        target_acc = float(input(\"Enter target accuracy: \"))\n",
    "\n",
    "    results = run_batch_language_detection(texts, true_labels, query_type, target_acc)\n",
    "    selected_model = ollama_select_model(results, query_type, target_acc)\n",
    "\n",
    "    print(\"Ollama Selected Model:\", selected_model)\n"
   ],
   "id": "8837b9ede6eaed07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "\n",
      "Query options:\n",
      "1 - Most accurate model\n",
      "2 - Fastest model at target accuracy\n",
      "Ollama Selected Model: langid.py\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
